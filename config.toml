# =============================================================================
# Before you start changing anything here, read the comments.
# All of them can be found below in the "DEFAULT" section

[DEFAULT]
    [DEFAULT.data]
        # The directory that contains extracted files of everything you've downloaded.
        data_dir = "data"

        # Train, dev and test jsonlines
        train_data = "data/english_train_head.jsonlines"
        dev_data = "data/english_development_head.jsonlines"
        test_data = "data/english_test_head.jsonlines"

    [DEFAULT.model_params]
        # Bert settings ======================

        # Base bert model architecture and tokenizer
        bert_model = "bert-large-cased"

        # Controls max length of sequences passed through bert to obtain its
        # contextual embeddings
        # Must be less than or equal to 512
        bert_window_size = 512

        # General model settings =============

        # Coref model name
        coref_model = "base"

        # Controls the dimensionality of feature embeddings
        embedding_size = 20

        # Controls the dimensionality of distance embeddings used by SpanPredictor
        sp_embedding_size = 64

        # Controls the number of spans for which anaphoricity can be scores in one
        # batch. Only affects final scoring; mention extraction and rough scoring
        # are less memory intensive, so they are always done in just one batch.
        a_scoring_batch_size = 512

        # AnaphoricityScorer FFNN parameters
        hidden_size = 1024
        n_hidden_layers = 1


        # Mention extraction settings ========

        # Mention extractor will check spans up to max_span_len words
        # The default value is chosen to be big enough to hold any dev data span
        max_span_len = 64


        # Pruning settings ===================

        # Controls how many pairs should be preserved per mention
        # after applying rough scoring.
        rough_k = 50

    [DEFAULT.training_params]
        # Training settings ==================
        # The device where everything is to be placed. "cuda:N"/"cpu" are supported.
        device = "cuda"

        # Controls whether to fine-tune bert_model
        bert_finetune = true

        # Controls the dropout rate throughout all models
        dropout_rate = 0.3

        # Bert learning rate (only used if bert_finetune is set)
        bert_learning_rate = 1e-5

        # Task learning rate
        learning_rate = 3e-4

        # For how many epochs the training is done
        train_epochs = 20

        # Controls the weight of binary cross entropy loss added to nlml loss
        bce_loss_weight = 0.5

        # The directory that will contain conll prediction files
        conll_log_dir = "data/conll_logs"

    [DEFAULT.metrics]
        # Metrics paramets
        [DEFAULT.metrics.pavpu]
            # PAVPU metric taken from https://arxiv.org/pdf/1811.12709.pdf

            # If sliding threshold is activated, static threshold value is skipped.
            # Sliding threshold samples a consecutive range of data in between [0,1]
            # and calculates PAVPU for every threshold value.
            # If sliding_threshold == false, static_theshold_value is used.
            sliding_threshold = false
            static_theshold_value = 0.5


    [DEFAULT.logging]
        # Set up for logging
        log_file = "coref_model_logs.log"
        stream_format = "%(asctime)s - %(levelname)s - %(name)s - %(message)s"
        datetime_format = "%Y-%m-%dT%H:%M:%S%z"
        verbosity = "debug"


    [DEFAULT.active_learning]
        # Active Learning parameters
        # Number of neural network parameters that will represent NN empirical distribution
        parameters_samples = 10

        [DEFAULT.active_learning.simulation]
            # Simulation parameters (e.g. starting sample size, number of iterations, ...)

            # Number of instances used for the first training iteration
            initial_sample_size = 100

            # Active learning steps to perform
            active_learning_steps = 5


        [DEFAULT.active_learning.sampling_strategy]
            # Active Learning sampling strategy. All AL related things will be put to a
            # dedicated section later
            # The function name to sample new data
            acquisition_function_type = "random"

            # Batch size to sample
            batch_size = 50

            # Prob[random_strategy|current_sampling_iteration/total_number_of_iterations] = 0.5
            # In other words strategy flip is the number of iterations after which the acquisition function
            # strategy will have higher probability to be chosen
            strategy_flip = 0.3

            # the total number of planned samplings
            total_number_of_iterations = 10

    [DEFAULT.manifold_learning]
        enable = true
        # Manifold Learning parameters
        # Loss function to use
        loss_name = "sq_rec_loss"
        loss_alpha = 5e-6
        # Metrics to display
        verbose_outputs = ["loss"]
        reduction_ratio = 0.75
        [DEFAULT.manifold_learning.standalone]
            # For separate (non-CR) use cases
            input_dimensionality = 0
            output_dimensionality = 0
            batch_size = 32
            shuffle = true
            learning_rate = 1e-2
            epochs = 10


    # =============================================================================
    # Extra keyword arguments to be passed to bert tokenizers of specified models
    [DEFAULT.tokenizer_kwargs]
        [DEFAULT.tokenizer_kwargs.roberta-large]
            "add_prefix_space" = true

        [DEFAULT.tokenizer_kwargs.spanbert-large-cased]
            "do_lower_case" = false

        [DEFAULT.tokenizer_kwargs.bert-large-cased]
            "do_lower_case" = false

# =============================================================================
# The sections listed here do not need to make use of all config variables
# If a variable is omitted, its default value will be used instead

[roberta]
    [roberta.model_params]
        bert_model = "roberta-large"

[roberta_no_bce]
    [roberta_no_bce.model_params]
        bert_model = "roberta-large"
    [roberta_no_bce.training_params]
        bce_loss_weight = 0.0

[spanbert]
    [spanbert.model_params]
        bert_model = "SpanBERT/spanbert-large-cased"

[spanbert_no_bce]
    [spanbert_no_bce.model_params]
        bert_model = "SpanBERT/spanbert-large-cased"
    [spanbert_no_bce.training_params]
        bce_loss_weight = 0.0

[bert]
    [bert.model_params]
        bert_model = "bert-large-cased"

[longformer]
    [longformer.model_params]
        bert_model = "allenai/longformer-large-4096"
        bert_window_size = 2048

[debug]
    [debug.data]
        # path to the test file for a pipeline test
        test_data = "data/english_pipeline_test_head.jsonlines"
    [debug.model_params]
        bert_window_size = 384
    [debug.training_params]
        device = "cpu"
        bert_finetune = false
    [debug.active_learning.sampling_strategy]
        batch_size = 1
        strategy_flip = 0.2
        total_number_of_iterations = 10
    [debug.metrics.pavpu]
        sliding_threshold = true
        static_theshold_value = 0.5
    [debug.manifold_learning]
        enable = true
        # Manifold Learning parameters
        loss_name = "sq_rec_loss"
        verbose_outputs = ["loss"]
        [debug.manifold_learning.standalone]
            input_dimensionality = 10
            output_dimensionality = 2
            batch_size = 32
            shuffle = true
            learning_rate = 1e-2
            epochs = 15
    [debug.logging]
        log_file = "test_logs.log"

[mc_dropout]
    [mc_dropout.model_params]
        bert_model = "roberta-large"
        coref_model = "mc_dropout"


# Reduced-dimensionality model configuration to overwrite
[reduced_dimensionality]
    [reduced_dimensionality.model_params]
        bert_model = "roberta-large"
        coref_model = "reduced_dimensionality"
    [reduced_dimensionality.training_params]
        device = "cuda"

[debug_gpu]
    [debug_gpu.model_params]
        bert_window_size = 384
    [debug_gpu.training_params]
        bert_finetune = false
