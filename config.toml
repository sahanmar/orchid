# =============================================================================
# Before you start changing anything here, read the comments.
# All of them can be found below in the "DEFAULT" section

[DEFAULT]
    [DEFAULT.data]
        # The directory that contains extracted files of everything you've downloaded.
        data_dir = "data"

        # Train, dev and test jsonlines
        train_data = "data/english_train_head.jsonlines"
        dev_data = "data/english_development_head.jsonlines"
        test_data = "data/english_test_head.jsonlines"

    [DEFAULT.model_params]
        # Bert settings ======================

        # Base bert model architecture and tokenizer
        bert_model = "bert-large-cased"

        # Controls max length of sequences passed through bert to obtain its
        # contextual embeddings
        # Must be less than or equal to 512
        bert_window_size = 512

        # General model settings =============

        # Coref model name
        coref_model = "base"

        # Controls the dimensionality of feature embeddings
        embedding_size = 20

        # Controls the dimensionality of distance embeddings used by SpanPredictor
        sp_embedding_size = 64

        # Controls the number of spans for which anaphoricity can be scores in one
        # batch. Only affects final scoring; mention extraction and rough scoring
        # are less memory intensive, so they are always done in just one batch.
        a_scoring_batch_size = 512

        # AnaphoricityScorer FFNN parameters
        hidden_size = 1024
        n_hidden_layers = 1


        # Mention extraction settings ========

        # Mention extractor will check spans up to max_span_len words
        # The default value is chosen to be big enough to hold any dev data span
        max_span_len = 64


        # Pruning settings ===================

        # Controls how many pairs should be preserved per mention
        # after applying rough scoring.
        rough_k = 50

    [DEFAULT.training_params]
        # Training settings ==================
        # The device where everything is to be placed. "cuda:N"/"cpu" are supported.
        device = "cuda"

        # Controls whether to fine-tune bert_model
        bert_finetune = true

        # Controls the dropout rate throughout all models
        dropout_rate = 0.3

        # Bert learning rate (only used if bert_finetune is set)
        bert_learning_rate = 1e-5

        # Task learning rate
        learning_rate = 3e-4

        # For how many epochs the training is done
        train_epochs = 20

        # Controls the weight of binary cross entropy loss added to nlml loss
        bce_loss_weight = 0.5

        # The directory that will contain conll prediction files
        conll_log_dir = "data/conll_logs"

    [DEFAULT.metrics]
        # Metrics paramets
        [DEFAULT.metrics.pavpu]
            # PAVPU metric taken from https://arxiv.org/pdf/1811.12709.pdf

            # If sliding threshold is activated, static threshold value is skipped.
            # Sliding threshold samples a consecutive range of data in between [0,1]
            # and calculates PAVPU for every threshold value.
            # If sliding_threshold == false, static_theshold_value is used.
            sliding_threshold = false
            static_theshold_value = 0.5


    [DEFAULT.logging]
        # Set up for logging
        logger_name = "coref-model"
        log_folder = "data/logs/"
        datetime_format = "%Y-%m-%dT%H:%M:%S%z"
        verbosity = "debug"
        stream_format = "%(asctime)s - %(levelname)s - %(name)s - %(message)s"
        [DEFAULT.logging.jsonl_format]
            timestamp="asctime"
            level="levelname"
            message="message"
            loggerName="name"
            processName="processName"
            processID="process"
            threadName="threadName"
            threadID="thread"

    [DEFAULT.active_learning]
        # Active Learning parameters
        # Number of neural network parameters that will represent NN empirical distribution
        parameters_samples = 10

        # Instance type to sample.
        # document - sample the document to train
        # random_token - random tokens sampling from documents to train
        # random_mention - random mentions samling from rough_scorer prediction to train
        # entropy_mention - entropy mentions samling from rough_scorer prediction to train
        # hac_entropy_mention - hac entropy mentions samling from rough_scorer prediction to train
        instance_sampling = "random_token"

        # Cold start means a fresh initialization of the weights in every iteration
        cold_start = true

        # Sampling stratedy
        strategy = "naive_sampling" # options: {naive_sampling, greedy_sampling}

        [DEFAULT.active_learning.simulation]
            # Simulation parameters (e.g. starting sample size, number of iterations, ...)

            # Number of instances used for the first training iteration
            initial_sample_size = 100

            # Active learning steps to perform
            active_learning_loops = 5


        [DEFAULT.active_learning.sampling_strategy]
            [DEFAULT.active_learning.sampling_strategy.naive_sampling]
                # Active Learning sampling strategy based on naive sampling without any adaptive behaviour.
                # This is just an abstraction on the top of direct usage of acquisition function with config
                # values

                # Batch size to sample
                batch_size = 50

                # the total number of planned samplings
                total_number_of_iterations = 10

                # Prioritize taking tokens from 0th to docs_of_interest-th idex, given the batch size
                docs_of_interest = 100


            [DEFAULT.active_learning.sampling_strategy.greedy_sampling]
                # Active Learning sampling strategy based on greedy sampling

                # Batch size to sample
                batch_size = 50

                # Prob[random_strategy|current_sampling_iteration/total_number_of_iterations] = 0.5
                # In other words strategy flip is the number of iterations after which the acquisition function
                # strategy will have higher probability to be chosen
                strategy_flip = 0.3

                # the total number of planned samplings
                total_number_of_iterations = 10

                # Prioritize taking tokens from 0th to docs_of_interest-th idex, given the batch size
                docs_of_interest = 100


    [DEFAULT.manifold_learning]
        enable = false
        # Manifold Learning parameters
        # Loss function to use
        loss_name = "sq_rec_loss"
        loss_in_forward = true
        loss_alpha = 1e-6
        # Metrics to display
        verbose_outputs = ["loss"]
        reduction_ratio = 0.75
        [DEFAULT.manifold_learning.standalone]
            # For separate (non-CR) use cases
            input_dimensionality = 0
            output_dimensionality = 0
            batch_size = 32
            shuffle = true
            learning_rate = 1e-2
            epochs = 10


    # =============================================================================
    # Extra keyword arguments to be passed to bert tokenizers of specified models
    [DEFAULT.tokenizer_kwargs]
        [DEFAULT.tokenizer_kwargs.roberta-large]
            "add_prefix_space" = true

        [DEFAULT.tokenizer_kwargs.spanbert-large-cased]
            "do_lower_case" = false

        [DEFAULT.tokenizer_kwargs.bert-large-cased]
            "do_lower_case" = false

# =============================================================================
# The sections listed here do not need to make use of all config variables
# If a variable is omitted, its default value will be used instead

[roberta]
    [roberta.model_params]
        bert_model = "roberta-large"

[roberta_no_bce]
    [roberta_no_bce.model_params]
        bert_model = "roberta-large"
    [roberta_no_bce.training_params]
        bce_loss_weight = 0.0

[spanbert]
    [spanbert.model_params]
        bert_model = "SpanBERT/spanbert-large-cased"

[spanbert_no_bce]
    [spanbert_no_bce.model_params]
        bert_model = "SpanBERT/spanbert-large-cased"
    [spanbert_no_bce.training_params]
        bce_loss_weight = 0.0

[bert]
    [bert.model_params]
        bert_model = "bert-large-cased"

[longformer]
    [longformer.model_params]
        bert_model = "allenai/longformer-large-4096"
        bert_window_size = 2048

[debug]
    [debug.data]
        # path to the test file for a pipeline test
        test_data = "data/english_pipeline_test_head.jsonlines"
    [debug.model_params]
        bert_window_size = 384
    [debug.training_params]
        device = "cpu"
        bert_finetune = false
    [debug.active_learning]
        instance_sampling = "random_token"
        strategy = "greedy_sampling"
    [debug.active_learning.sampling_strategy.greedy_sampling]
        batch_size = 1
        strategy_flip = 0.2
        total_number_of_iterations = 10
    [debug.metrics.pavpu]
        sliding_threshold = true
        static_theshold_value = 0.5
    [debug.manifold_learning]
        enable = true
        # Manifold Learning parameters
        loss_name = "sq_rec_loss"
        loss_in_forward = true
        verbose_outputs = ["loss"]
        [debug.manifold_learning.standalone]
            input_dimensionality = 10
            output_dimensionality = 2
            batch_size = 32
            shuffle = true
            learning_rate = 1e-2
            epochs = 15
    [debug.logging]
        logger_name = "test_run"


[debug_active_learning]
    [debug_active_learning.logging]
        logger_name = "test_run"
    [debug_active_learning.data]
        # path to the test file for a pipeline test
        test_data = "data/english_pipeline_test_head.jsonlines"
    [debug_active_learning.model_params]
        bert_window_size = 384
    [debug_active_learning.training_params]
        device = "cpu"
        bert_finetune = false
    [debug_active_learning.active_learning]
        instance_sampling = "random_mention"
        strategy = "naive_sampling"


[mc_dropout]
    [mc_dropout.model_params]
        bert_model = "roberta-large"
        coref_model = "mc_dropout"


# Reduced-dimensionality model configuration to overwrite
[reduced_dimensionality]
    [reduced_dimensionality.model_params]
        bert_model = "roberta-large"
        coref_model = "reduced_dimensionality"
    [reduced_dimensionality.training_params]
        device = "cuda"
    [reduced_dimensionality.manifold_learning]
        enable = true
        loss_alpha = 5e-6
        reduction_ratio = 0.75

[debug_gpu]
    [debug_gpu.model_params]
        bert_window_size = 384
    [debug_gpu.training_params]
        bert_finetune = false

# Active Learning
[active_learning_simulation]
    [active_learning_simulation.model_params]
        coref_model = "mc_dropout"
        bert_model = "roberta-large"
    [active_learning_simulation.active_learning]
        instance_sampling = "entropy_mention" # "random_mention"
        strategy = "naive_sampling" # greedy_sampling
        cold_start = false
    [active_learning_simulation.active_learning.sampling_strategy.naive_sampling]
        batch_size = 20000
        docs_of_interest = 200
        total_number_of_iterations = 50
    [active_learning_simulation.training_params]
        train_epochs = 10
    [active_learning_simulation.active_learning.simulation]
        active_learning_loops = 2
