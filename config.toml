# =============================================================================
# Before you start changing anything here, read the comments.
# All of them can be found below in the "DEFAULT" section

[DEFAULT]
    [DEFAULT.data]
        # The directory that contains extracted files of everything you've downloaded.
        data_dir = "data"

        # Train, dev and test jsonlines
        train_data = "data/english_train_head.jsonlines"
        dev_data = "data/english_development_head.jsonlines"
        test_data = "data/english_test_head.jsonlines"

    [DEFAULT.model_params]
        # Bert settings ======================

        # Base bert model architecture and tokenizer
        bert_model = "bert-large-cased"

        # Controls max length of sequences passed through bert to obtain its
        # contextual embeddings
        # Must be less than or equal to 512
        bert_window_size = 512

        # General model settings =============

        # Coref model name
        coref_model = "base"

        # Controls the dimensionality of feature embeddings
        embedding_size = 20

        # Controls the dimensionality of distance embeddings used by SpanPredictor
        sp_embedding_size = 64

        # Controls the number of spans for which anaphoricity can be scores in one
        # batch. Only affects final scoring; mention extraction and rough scoring
        # are less memory intensive, so they are always done in just one batch.
        a_scoring_batch_size = 512

        # AnaphoricityScorer FFNN parameters
        hidden_size = 1024
        n_hidden_layers = 1


        # Mention extraction settings ========

        # Mention extractor will check spans up to max_span_len words
        # The default value is chosen to be big enough to hold any dev data span
        max_span_len = 64


        # Pruning settings ===================

        # Controls how many pairs should be preserved per mention
        # after applying rough scoring.
        rough_k = 50

    [DEFAULT.training_params]
        # Training settings ==================
        # The device where everything is to be placed. "cuda:N"/"cpu" are supported.
        device = "cuda"

        # Controls whether to fine-tune bert_model
        bert_finetune = true

        # Controls the dropout rate throughout all models
        dropout_rate = 0.3

        # Bert learning rate (only used if bert_finetune is set)
        bert_learning_rate = 1e-5

        # Task learning rate
        learning_rate = 3e-4

        # For how many epochs the training is done
        train_epochs = 20

        # Controls the weight of binary cross entropy loss added to nlml loss
        bce_loss_weight = 0.5

        # The directory that will contain conll prediction files
        conll_log_dir = "data/conll_logs"

    [DEFAULT.metrics]
        # Metrics paramets
        [DEFAULT.metrics.pavpu]
            # PAVPU metric taken from https://arxiv.org/pdf/1811.12709.pdf

            # If sliding threshold is activated, static threshold value is skipped.
            # Sliding threshold samples a consecutive range of data in between [0,1]
            # and calculates PAVPU for every threshold value.
            # If sliding_threshold == false, static_theshold_value is used.
            sliding_threshold = false
            static_theshold_value = 0.5



    [DEFAULT.active_learning]
        # Active Learning parameters
        # Number of neural network parameters that will represent NN empirical distribution
        parameters_samples = 10

        [DEFAULT.active_learning.simulation]
            # Simulation parameters (e.g. starting sample size, number of iterations, ...)

            # Number of instances used for the first training iteration
            initial_sample_size = 100

            # Active learning steps to perform
            active_learning_steps = 5


        [DEFAULT.active_learning.sampling_strategy]
            # Active Learning sampling strategy. All AL related things will be put to a
            # dedicated section later
            # The function name to sample new data
            acquisition_function_type = "random"

            # Batch size to sample
            batch_size = 50

            # Prob[random_strategy|current_sampling_iteration/total_number_of_iterations] = 0.5
            # In other words strategy flip is the number of iterations after which the acquisition function
            # strategy will have higher probability to be chosen
            strategy_flip = 0.3

            # the total number of planned samplings
            total_number_of_iterations = 10


    # =============================================================================
    # Extra keyword arguments to be passed to bert tokenizers of specified models
    [DEFAULT.tokenizer_kwargs]
        [DEFAULT.tokenizer_kwargs.roberta-large]
            "add_prefix_space" = true

        [DEFAULT.tokenizer_kwargs.spanbert-large-cased]
            "do_lower_case" = false

        [DEFAULT.tokenizer_kwargs.bert-large-cased]
            "do_lower_case" = false

# =============================================================================
# The sections listed here do not need to make use of all config variables
# If a variable is omitted, its default value will be used instead

[roberta]
    [roberta.model_params]
        bert_model = "roberta-large"

[roberta_no_bce]
    [roberta_no_bce.model_params]
        bert_model = "roberta-large"
    [roberta_no_bce.training_params]
        bce_loss_weight = 0.0

[spanbert]
    [spanbert.model_params]
        bert_model = "SpanBERT/spanbert-large-cased"

[spanbert_no_bce]
    [spanbert_no_bce.model_params]
        bert_model = "SpanBERT/spanbert-large-cased"
    [spanbert_no_bce.training_params]
        bce_loss_weight = 0.0

[bert]
    [bert.model_params]
        bert_model = "bert-large-cased"

[longformer]
    [longformer.model_params]
        bert_model = "allenai/longformer-large-4096"
        bert_window_size = 2048

[debug]
    [debug.data]
        # path to the test file for a pipeline test
        test_data = "data/english_pipeline_test_head.jsonlines"
    [debug.model_params]
        bert_window_size = 384
    [debug.training_params]
        device = "cpu"
        bert_finetune = false
    [debug.active_learning.sampling_strategy]
        batch_size = 1
        strategy_flip = 0.2
        total_number_of_iterations = 10
    [debug.metrics.pavpu]
        sliding_threshold = true
        static_theshold_value = 0.5


[mc_dropout]
    [mc_dropout.model_params]
        bert_model = "roberta-large"
        coref_model = "mc_dropout"

[debug_gpu]
    [debug_gpu.model_params]
        bert_window_size = 384
    [debug_gpu.training_params]
        bert_finetune = false
